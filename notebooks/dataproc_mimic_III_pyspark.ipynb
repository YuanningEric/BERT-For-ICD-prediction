{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Processing With PySpark\n",
    "\n",
    "In this notebook, we refactor the data processing script to leverage the usage of big data tool (pysark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.path.append('../')\n",
    "import datasets\n",
    "from dataproc import extract_wvs\n",
    "from dataproc import get_discharge_summaries\n",
    "from dataproc import concat_and_split\n",
    "from dataproc import build_vocab\n",
    "from dataproc import vocab_index_descriptions\n",
    "from dataproc import word_embeddings\n",
    "from constants import MIMIC_3_DIR, DATA_DIR\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some contants for later use. PySpark configuration is also set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1573964629429'),\n",
       " ('spark.cores.max', '3'),\n",
       " ('spark.driver.port', '44139'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.app.name', 'NLP'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.1.109'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "Y = 'full' #use all available labels in the dataset for prediction\n",
    "notes_file = '%s/NOTEEVENTS.csv' % MIMIC_3_DIR # raw note events downloaded from MIMIC-III\n",
    "vocab_size = 'full' #don't limit the vocab size to a specific number\n",
    "vocab_min = 3 #discard tokens appearing in fewer than this many documents\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"NLP\").getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), \n",
    "                                        ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine diagnosis and procedure codes and reformat them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes in MIMIC-III are given in separate files for procedures and diagnoses, and the codes are given without periods, which might lead to collisions if we naively combine them. So we have to add the periods back in the right place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_procedure = spark.read.csv('%s/PROCEDURES_ICD.csv' % MIMIC_3_DIR, header=True)\n",
    "df_diagnoses = spark.read.csv('%s/DIAGNOSES_ICD.csv' % MIMIC_3_DIR, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get the information of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "240095\n",
      "651047\n"
     ]
    }
   ],
   "source": [
    "print(df_procedure.columns)\n",
    "print(df_diagnoses.columns)\n",
    "print(df_procedure.count())\n",
    "print(df_diagnoses.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top 5 rows of each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+---------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|SEQ_NUM|ICD9_CODE|\n",
      "+------+----------+-------+-------+---------+\n",
      "|   944|     62641| 154460|      3|     3404|\n",
      "|   945|      2592| 130856|      1|     9671|\n",
      "|   946|      2592| 130856|      2|     3893|\n",
      "|   947|     55357| 119355|      1|     9672|\n",
      "|   948|     55357| 119355|      2|     0331|\n",
      "+------+----------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+----------+-------+-------+---------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|SEQ_NUM|ICD9_CODE|\n",
      "+------+----------+-------+-------+---------+\n",
      "|  1297|       109| 172335|      1|    40301|\n",
      "|  1298|       109| 172335|      2|      486|\n",
      "|  1299|       109| 172335|      3|    58281|\n",
      "|  1300|       109| 172335|      4|     5855|\n",
      "|  1301|       109| 172335|      5|     4254|\n",
      "+------+----------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_procedure.show(5)\n",
    "df_diagnoses.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define UDF to process the column data and parse the ICD9 code to regular format (with period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reformat (code, file_type):\n",
    "    \"\"\"\n",
    "        Put a period in the right place because the MIMIC-3 data files exclude them.\n",
    "        Generally, procedure codes have dots after the first two digits, \n",
    "        while diagnosis codes have dots after the first three digits.\n",
    "    \"\"\"\n",
    "    if code is None:\n",
    "        return code\n",
    "    code = ''.join(code.split('.'))\n",
    "    if file_type == \"diagnoses\":\n",
    "        if code.startswith('E'):\n",
    "            if len(code) > 4:\n",
    "                code = code[:4] + '.' + code[4:]\n",
    "        else:\n",
    "            if len(code) > 3:\n",
    "                code = code[:3] + '.' + code[3:]\n",
    "    else:\n",
    "        code = str(int(code))\n",
    "        code = code[:2] + '.' + code[2:]\n",
    "    return code \n",
    "\n",
    "code_reformat_udf = F.udf(reformat, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+---------+-------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|SEQ_NUM|ICD9_CODE|absolute_code|\n",
      "+------+----------+-------+-------+---------+-------------+\n",
      "|  1297|       109| 172335|      1|    40301|       403.01|\n",
      "|  1298|       109| 172335|      2|      486|          486|\n",
      "|  1299|       109| 172335|      3|    58281|       582.81|\n",
      "|  1300|       109| 172335|      4|     5855|        585.5|\n",
      "|  1301|       109| 172335|      5|     4254|        425.4|\n",
      "+------+----------+-------+-------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_diagnoses = df_diagnoses.withColumn(\"absolute_code\", code_reformat_udf(df_diagnoses[\"ICD9_CODE\"], F.lit(\"diagnoses\")))\n",
    "df_procedure = df_procedure.withColumn(\"absolute_code\", code_reformat_udf(df_procedure[\"ICD9_CODE\"], F.lit(\"procedure\")))\n",
    "df_codes = df_diagnoses.union(df_procedure)\n",
    "df_codes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Export all codes to a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_codes.toPandas().to_csv('%s/pyspark/ALL_CODES.csv' % MIMIC_3_DIR, index=False,\n",
    "               columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
    "               header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of all codes after using the pyspark method. And compare with the outputs from Pandas ETL methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique code number from Pandas ETL Script:  8994\n",
      "Unique code number from PySpark ETL Script: 8994\n",
      "Number of different code:  1\n",
      "Different code:  {None}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#In the full dataset (not just discharge summaries)\n",
    "df = spark.read.csv('%s/pyspark/ALL_CODES.csv' % MIMIC_3_DIR, header=True)\n",
    "df_code_unique = set(df.select('ICD9_CODE').distinct().rdd.map(lambda x: x[0]).collect())\n",
    "df_reference = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, dtype={\"ICD9_CODE\": str})\n",
    "all_code = set(df_reference['ICD9_CODE'].unique())\n",
    "print(\"Unique code number from Pandas ETL Script: \", len(all_code))\n",
    "print(\"Unique code number from PySpark ETL Script:\", len(df_code_unique))\n",
    "print(\"Number of different code: \", len(df_code_unique.difference(all_code)))\n",
    "print(\"Different code: \", df_code_unique.difference(all_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and preprocess raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing time!\n",
    "\n",
    "In the original notebook, the discharge summaries were processed line by line without any dataset level processing. Here, we will use pyspark to process the data.\n",
    "\n",
    "First, we created a UDF to tokenize the discharge text information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#retain only alphanumeric\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize_discharge(note):\n",
    "    \"\"\"\n",
    "    This function will tokenize discharge summaries in the note column\n",
    "    It will tokenize discharge summarizes\n",
    "    :param note: text format note\n",
    "    :return: tokenized strings\n",
    "    \"\"\"\n",
    "    tokens = [t.lower() for t in tokenizer.tokenize(note) if not t.isnumeric()]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "tokenize_discharge_udf = F.udf(tokenize_discharge, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will process the discharge summary. \n",
    "This will:\n",
    "- Select only discharge summaries and their addenda\n",
    "- remove punctuation and numeric-only tokens, removing 500 but keeping 250mg\n",
    "- lowercase all tokens\n",
    "Then, we will save a copy of tokenized discharge file in outoput.\n",
    "To compare the performance to PySpark processing against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dischange dataframe schema:  StructType(List(StructField(ROW_ID,StringType,true),StructField(SUBJECT_ID,StringType,true),StructField(HADM_ID,StringType,true),StructField(CHARTDATE,StringType,true),StructField(CHARTTIME,StringType,true),StructField(STORETIME,StringType,true),StructField(CATEGORY,StringType,true),StructField(DESCRIPTION,StringType,true),StructField(CGID,StringType,true),StructField(ISERROR,StringType,true),StructField(TEXT,StringType,true)))\n",
      "Saving discharge summary to a file...\n",
      "Total Processing time:  55.37991976737976  seconds\n",
      "+----------+-------+---------+--------------------+\n",
      "|SUBJECT_ID|HADM_ID|CHARTTIME|                TEXT|\n",
      "+----------+-------+---------+--------------------+\n",
      "|     22532| 167853|     null|admission date di...|\n",
      "|     13702| 107527|     null|admission date di...|\n",
      "|     13702| 167118|     null|admission date di...|\n",
      "|     13702| 196489|     null|admission date di...|\n",
      "|     26880| 135453|     null|admission date di...|\n",
      "+----------+-------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_discharge = spark.read.option(\"multiLine\", True).option(\"escape\", \"\\\"\").\\\n",
    "                csv(\"%s/NOTEEVENTS.csv\" %MIMIC_3_DIR, header=True)\n",
    "start = time.time()\n",
    "df_discharge = df_discharge.filter(df_discharge['CATEGORY'] == \"Discharge summary\")\\\n",
    "    .withColumn(\"tokens\", tokenize_discharge_udf(df_discharge['TEXT']))\n",
    "print(\"Saving discharge summary to a file...\")\n",
    "df_discharge = df_discharge.select(\"SUBJECT_ID\",\"HADM_ID\",\"CHARTTIME\",\"tokens\")\n",
    "df_discharge = df_discharge.withColumnRenamed(\"tokens\", \"TEXT\")\n",
    "df_discharge.write.csv('%s/pyspark/hdfs/disch_full.csv' % MIMIC_3_DIR, header=True, mode=\"overwrite\")\n",
    "end = time.time()\n",
    "print(\"Total Processing time: \", end-start, \" seconds\")\n",
    "df_discharge.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the comparison of data processing time for the discharge summary file. Currently, the spark configuration is `('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')`\n",
    "\n",
    "| Original Script Processing time (sec) | PySpark Script Processing Time including  (sec) |\n",
    "| --- | ---|\n",
    "|66.5|55.4|\n",
    "\n",
    "Next, we will check the data processing quality by comparing with the original script output.\n",
    "\n",
    "Since the output from last step was written to hdfs, the file name has been changed. So in this step, we need to manual change the file name and file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique code number from original ETL Script:  52726\n",
      "Unique code number from PySpark ETL Script: 52726\n",
      "Number of different code:  0\n",
      "Different code:  set()\n"
     ]
    }
   ],
   "source": [
    "#How many admissions?\n",
    "df_discharge = spark.read.csv('%s/pyspark/disch_full.csv' % MIMIC_3_DIR, header=True)\n",
    "admission_set = set(df_discharge.select('HADM_ID').distinct().rdd.map(lambda x: x[0]).collect())\n",
    "\n",
    "df_original = pd.read_csv('%s/disch_full.csv' % MIMIC_3_DIR, dtype={\"HADM_ID\": str})\n",
    "original_admission_set = set(df_original['HADM_ID'].unique())\n",
    "\n",
    "print(\"Unique code number from original ETL Script: \", len(original_admission_set))\n",
    "print(\"Unique code number from PySpark ETL Script:\", len(admission_set))\n",
    "print(\"Number of different code: \", len(admission_set.difference(original_admission_set)))\n",
    "print(\"Different code: \", admission_set.difference(original_admission_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the data processing with count of tokens and unique token types. Output from original script was `num types 150853 num tokens 79801402`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  79801402\n",
      "Number of types:  150853\n"
     ]
    }
   ],
   "source": [
    "#Tokens and types\n",
    "df_discharge_explode = df_discharge.withColumn(\"TOKEN\", F.explode(F.split(df_discharge['TEXT'], \" \")) )\n",
    "print(\"Number of tokens: \", df_discharge_explode.count())\n",
    "print(\"Number of types: \", df_discharge_explode.select('TOKEN').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate labels with set of discharge summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will filter the code based on `HADM_ID` with discharge summaries, so we will exclude the Code records which doesn't have any discharge summaries asccociated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codes = spark.read.csv('%s/pyspark/ALL_CODES.csv' % MIMIC_3_DIR, header=True)\n",
    "df_codes_filtered = df_codes.filter(df_codes['HADM_ID'].isin(admission_set))\n",
    "df_codes_filtered.toPandas().to_csv('%s/pyspark/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index=False,\n",
    "               columns=['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE'],\n",
    "               header=['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can validate the data processing by comparing the number of unique HADM_IDs (original script has 52726 unqiue IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of HADM_IDs with both codes and discharge summary:  52726\n"
     ]
    }
   ],
   "source": [
    "df_codes_filtered = spark.read.csv('%s/pyspark/ALL_CODES_filtered.csv' % MIMIC_3_DIR, header=True)\n",
    "print(\"The number of HADM_IDs with both codes and discharge summary: \", \n",
    "      df_codes_filtered.select(\"HADM_ID\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append labels to notes in a single file\n",
    "\n",
    "In this step, we will append the ICD code label to another column in discharge summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|SUBJECT_ID|HADM_ID|              LABELS|\n",
      "+----------+-------+--------------------+\n",
      "|     10661| 139315|13.9;11.4;38.93;3...|\n",
      "|      1086| 114240|431;584.9;425.4;5...|\n",
      "|     11604| 178435|431;401.9;433.10;...|\n",
      "|     11657| 103198|34.91;96.71;96.04...|\n",
      "|     11691| 138190|96.6;38.93;577.0;...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_label_agg = df_codes_filtered.groupby(\"SUBJECT_ID\", \"HADM_ID\").\\\n",
    "               agg(F.concat_ws(\";\", F.collect_list(df_codes_filtered[\"ICD9_CODE\"])).alias(\"LABELS\"))\n",
    "df_label_agg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52726\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|SUBJECT_ID|HADM_ID|                TEXT|              LABELS|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|     10661| 139315|admission date di...|13.9;11.4;38.93;3...|\n",
      "|      1086| 114240|admission date di...|431;584.9;425.4;5...|\n",
      "|     11604| 178435|admission date di...|431;401.9;433.10;...|\n",
      "|     11657| 103198|admission date di...|34.91;96.71;96.04...|\n",
      "|     11691| 138190|admission date di...|96.6;38.93;577.0;...|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_discharge = spark.read.csv('%s/pyspark/disch_full.csv' % MIMIC_3_DIR, header=True)\n",
    "df_discharge_labeled = df_discharge.join(df_label_agg, (df_discharge['SUBJECT_ID'] == df_label_agg['SUBJECT_ID']) & \\\n",
    "                                        ((df_discharge['HADM_ID'] == df_label_agg['HADM_ID'])), how=\"left\").\\\n",
    "                                         select(df_discharge.SUBJECT_ID, df_discharge.HADM_ID, \"TEXT\", \"LABELS\")\n",
    "print(df_discharge_labeled.select('HADM_ID').distinct().count())\n",
    "df_discharge_labeled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge_labeled.coalesce(1).\\\n",
    "            write.csv('%s/pyspark/hdfs/notes_labeled.csv' % MIMIC_3_DIR, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/dev/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLITTING\n",
      "0 read\n",
      "10000 read\n",
      "20000 read\n",
      "30000 read\n",
      "40000 read\n",
      "50000 read\n"
     ]
    }
   ],
   "source": [
    "fname = '%s/notes_labeled.csv' % MIMIC_3_DIR\n",
    "base_name = \"%s/disch\" % MIMIC_3_DIR #for output\n",
    "tr, dv, te = concat_and_split.split_data(fname, base_name=base_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data...\n",
      "removing rare terms\n",
      "51917 terms qualify out of 140794 total\n",
      "writing output\n"
     ]
    }
   ],
   "source": [
    "vocab_min = 3\n",
    "vname = '%s/vocab.csv' % MIMIC_3_DIR\n",
    "build_vocab.build_vocab(vocab_min, tr, vname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sort each data split by length for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/disch_%s_split.csv' % (MIMIC_3_DIR, splt)\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_full.csv' % (MIMIC_3_DIR, splt), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train word embeddings on all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building word2vec vocab on /home/chaopu/CS6250_BD4H/Final_Projects/caml-mimic/caml-mimic/mimicdata/mimic3/disch_full.csv...\n",
      "training...\n",
      "writing embeddings to /home/chaopu/CS6250_BD4H/Final_Projects/caml-mimic/caml-mimic/mimicdata/mimic3/processed_full.w2v\n"
     ]
    }
   ],
   "source": [
    "##https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html\n",
    "\n",
    "w2v_file = word_embeddings.word_embeddings('full', '%s/disch_full.csv' % MIMIC_3_DIR, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pre-trained word embeddings with new vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51917/51917 [00:00<00:00, 207286.27it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_wvs.gensim_to_embeddings('%s/processed_full.w2v' % MIMIC_3_DIR, '%s/vocab.csv' % MIMIC_3_DIR, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process code descriptions using the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22267/22267 [00:00<00:00, 130775.23it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_index_descriptions.vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,\n",
    "                                                  '%s/description_vectors.vocab' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter each split to the top 50 diagnosis/procedure codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|LABEL_SINGLE|count|\n",
      "+------------+-----+\n",
      "|       401.9|20053|\n",
      "|       38.93|14444|\n",
      "|       428.0|12842|\n",
      "|      427.31|12594|\n",
      "|      414.01|12179|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first calculate the top k\n",
    "df_nl = spark.read.csv('%s/notes_labeled.csv' % MIMIC_3_DIR, header=True)\n",
    "df_nl_explode = df_nl.select(\"LABELS\").withColumn(\"LABEL_SINGLE\", F.explode(F.split(df_nl[\"LABELS\"], \";\"))).\\\n",
    "                select(\"LABEL_SINGLE\")\n",
    "df_nl_explode = df_nl_explode.groupBy(\"LABEL_SINGLE\").count().orderBy(\"count\", ascending=False)\n",
    "df_nl_explode.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_topK = df_nl_explode.limit(topK).select(\"LABEL_SINGLE\").rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('%s/pyspark/TOP_%s_CODES.csv' % (MIMIC_3_DIR, str(topK)), 'w') as of:\n",
    "    w = csv.writer(of)\n",
    "    for code in code_topK:\n",
    "        w.writerow([code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    print(splt)\n",
    "    hadm_ids = set()\n",
    "    with open('%s/%s_50_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
    "        for line in f:\n",
    "            hadm_ids.add(line.rstrip())\n",
    "    \n",
    "    with open('%s/notes_labeled.csv' % MIMIC_3_DIR, 'r') as f:\n",
    "        with open('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(topK)), 'w') as of:\n",
    "            r = csv.reader(f)\n",
    "            w = csv.writer(of)\n",
    "            #header\n",
    "            w.writerow(next(r))\n",
    "            i = 0\n",
    "            for row in r:\n",
    "                hadm_id = row[1]\n",
    "                if hadm_id not in hadm_ids:\n",
    "                    continue\n",
    "                codes = set(str(row[3]).split(';'))\n",
    "                filtered_codes = codes.intersection(set(code_topK))\n",
    "                if len(filtered_codes) > 0:\n",
    "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate the add a column `length` with the length of tokens in each record, and then order by this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(text):\n",
    "    word_list = text.split(\" \")\n",
    "    return len(word_list)\n",
    "count_token_udf = F.udf(count_token, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(topK))\n",
    "    df = spark.read.csv(filename, header=True)\n",
    "    df = df.withColumn(\"length\", count_token_udf(df['TEXT'])).orderBy(\"length\")\n",
    "    df.toPandas().to_csv('%s/pyspark/%s_%s.csv' % (MIMIC_3_DIR, splt, str(topK)), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}